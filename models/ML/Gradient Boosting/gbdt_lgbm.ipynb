{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Package imports ###\n",
    "\n",
    "#from ray import tune\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import darts \n",
    "from darts.models import RegressionModel\n",
    "from lightgbm import LGBMRegressor as lgbm\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#AutoPatchTST\n",
    "\n",
    "### Ignore warnings ###\n",
    "\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "os.environ[\"TUNE_DISABLE_STRICT_METRIC_CHECKING\"] = \"1\"\n",
    "\n",
    "### Data preprocessing ###\n",
    "\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = (pd.read_csv(file_path)\n",
    "          .rename(columns={\"year_quarter\": \"ds\", \"GDPC1\": \"y\"})\n",
    "          .assign(unique_id=np.ones(len(pd.read_csv(file_path))),\n",
    "                  ds=lambda df: pd.to_datetime(df['ds'])))\n",
    "    columns_order = [\"unique_id\", \"ds\", \"y\"] + \\\n",
    "        [col for col in df.columns if col not in [\"unique_id\", \"ds\", \"y\"]]\n",
    "    #df['ds'] = df['ds'] - pd.Timedelta(days=1)\n",
    "    return df[columns_order]\n",
    "\n",
    "\n",
    "def separate_covariates(df, point_in_time):\n",
    "    covariates = df.drop(columns=[\"unique_id\", \"ds\", \"y\"])\n",
    "\n",
    "    if not point_in_time:\n",
    "        return df[covariates.columns], df[[]]\n",
    "\n",
    "    mask = covariates.apply(\n",
    "        lambda col: col.loc[col.index >= point_in_time - 1].isnull().any())\n",
    "\n",
    "    past_covariates = df[mask.index[mask]]\n",
    "    future_covariates = df[mask.index[~mask]]\n",
    "\n",
    "    return past_covariates, future_covariates\n",
    "\n",
    "\n",
    "def impute_missing_values_interpolate(data, method='linear'):\n",
    "    imputed_data = data.copy()\n",
    "    imputed_data.fillna(method='bfill', inplace=True)\n",
    "    return imputed_data.interpolate(method=method)\n",
    "\n",
    "def variable_screening(p,n,target_df,pc_df,fc_df):\n",
    "#We screen variables based on the first n quarters.    \n",
    "#We compute the correlation between the first n entries of the target_df and the covariates which are \n",
    "#separated into past and future covariates according to the separate_covariates function    \n",
    "    target_tr_df = np.array(target_df.iloc[:n,2])\n",
    "    pc_tr_df = np.array(pc_df.iloc[:n,:])\n",
    "    fc_tr_df = np.array(fc_df.iloc[:n,:])\n",
    "\n",
    "    pc_cors = np.array([np.abs(np.corrcoef(target_tr_df,pc_tr_df[:,i])[0,1]) for i in range(pc_tr_df.shape[1])])\n",
    "    fc_cors = np.array([np.abs(np.corrcoef(target_tr_df,fc_tr_df[:,i])[0,1]) for i in range(fc_tr_df.shape[1])])\n",
    "\n",
    "#Throw away all variables with absolute correlation <= p. The past and/or future covariates could be empty.\n",
    "    pc_ind = np.where(pc_cors > p)[0]\n",
    "    pc_empty = np.any(pc_ind)\n",
    "    fc_ind = np.where(fc_cors > p)[0]\n",
    "    fc_empty = np.any(fc_ind)\n",
    "\n",
    "    np_pc = np.array(pc_df)[:,pc_ind]\n",
    "    np_fc = np.array(fc_df)[:,fc_ind]\n",
    "\n",
    "#convert everything to Darts time series objects. I always throw away the last entry in the target series...This corresponds \n",
    "#to the value we want to forecaast.\n",
    "    target_series = darts.TimeSeries.from_times_and_values(times=pd.DatetimeIndex(target_df[\"ds\"]),values=np.array(target_df[\"y\"]))\n",
    "#the .drop_after method throws way everything after and including the split point\n",
    "    target_series = target_series.drop_after(target_series.time_index[len(target_series)-1])\n",
    "    \n",
    "    if(pc_empty): pc_series = darts.TimeSeries.from_times_and_values(times=pd.DatetimeIndex(target_df[\"ds\"]),values=np_pc)\n",
    "    else: pc_series = []\n",
    "\n",
    "    if(fc_empty): fc_series =  darts.TimeSeries.from_times_and_values(times=pd.DatetimeIndex(target_df[\"ds\"]),values=np_fc)\n",
    "    else: fc_series = []\n",
    "\n",
    "    return target_series, pc_series, fc_series, pc_empty, fc_empty\n",
    "\n",
    "vintage_files = [\n",
    "    f'../../../data/FRED/blocked/vintage_{year}_{month:02d}.csv'\n",
    "    for year in range(2018, 2024)\n",
    "    for month in range(1, 13)\n",
    "    if not (\n",
    "        (year == 2018 and month < 5) or\n",
    "        (year == 2023 and month > 5)\n",
    "    )\n",
    "] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_vintage(vintage_file):\n",
    "   \n",
    "    df = load_data(vintage_file)\n",
    "    target_df = df[[\"unique_id\", \"ds\", \"y\"]]\n",
    "    \n",
    "\n",
    "    point_in_time = df.index[-2] # explain later\n",
    "    target_date = target_df['ds'].iloc[-1]\n",
    "    \n",
    "    past_covariates, future_covariates = separate_covariates(df, point_in_time)\n",
    "\n",
    "    pc_df = impute_missing_values_interpolate(past_covariates)\n",
    "    fc_df = impute_missing_values_interpolate(future_covariates)\n",
    "#I assign nan to the last values of the target series and past covariates.\n",
    "#Just a check against any sort of data leakage.\n",
    "    target_df.iloc[target_df.shape[0]-1,2] = np.nan\n",
    "    pc_df.iloc[target_df.shape[0]-1,:] = np.nan\n",
    "\n",
    "#select 20% of the data for validation\n",
    "    val_size = int(0.2*(pc_df.shape[0]-1))\n",
    "    tr_size = pc_df.shape[0] -1 - val_size\n",
    "    date_time = pd.DatetimeIndex(target_df[\"ds\"])\n",
    "    val_start = date_time[tr_size]\n",
    "\n",
    "#Initialize the grids for the hyper-parameter tuning.\n",
    "    target_lags = [-1,-2,-3]\n",
    "    pc_lags = [-1,-2]\n",
    "    fc_lags = [0,-1]\n",
    "    lr_grid = [0.1,0.2,0.3]\n",
    "    ntrees_grid = [25,50,100,200]\n",
    "    s_grid = [0.3,0.4,0.5,0.6,0.7]\n",
    "    err_mat = np.ones([len(target_lags),len(pc_lags),len(fc_lags),len(lr_grid),len(ntrees_grid),len(s_grid)])\n",
    "    \n",
    "    for t in range(err_mat.shape[0]):\n",
    "        for p in range(err_mat.shape[1]):\n",
    "            for f in range(err_mat.shape[2]):\n",
    "                for l in range(err_mat.shape[3]):\n",
    "                    for nt in range(err_mat.shape[4]):\n",
    "                        for s in range(err_mat.shape[5]):\n",
    "                            target_series, pc_series, fc_series, pc_empty, fc_empty = variable_screening(p=s_grid[s],n=tr_size,target_df=target_df,pc_df=pc_df,fc_df=fc_df)\n",
    "                            reg_model = lgbm(learning_rate=lr_grid[l],n_estimators=ntrees_grid[nt],n_jobs=10,num_leaves=6)\n",
    "\n",
    "                            if(pc_empty & fc_empty):\n",
    "                            #Our base model is SVR where all the covariates and response are normalized. \n",
    "                            #I used the sklearn pipe to avoid any data leakage.\n",
    "                                model = RegressionModel(lags=target_lags[:(t+1)], lags_future_covariates = fc_lags[:(f+1)],\n",
    "                                                        lags_past_covariates=pc_lags[:(p+1)],output_chunk_length=1,\n",
    "                                                        model=reg_model)\n",
    "                                \n",
    "                            #Here I compute the historical forecasts, backtesting, temporal cross-validation, whatever you want to call it.\n",
    "                            #Note that the historical forecasts are computed over a period that was not used in the variable screening process.\n",
    "                                temp_cv = model.historical_forecasts(series=target_series, past_covariates=pc_series, future_covariates=fc_series, \n",
    "                                                       start=val_start, forecast_horizon=1, stride=1, retrain=True,verbose=False)\n",
    "\n",
    "                            if(pc_empty == True & fc_empty == False):\n",
    "                            #case when there are no future covariates\n",
    "                                model = RegressionModel(lags=target_lags[:(t+1)], lags_future_covariates = fc_lags[:(f+1)],\n",
    "                                                        lags_past_covariates=pc_lags[:(p+1)],output_chunk_length=1,\n",
    "                                                        model=reg_model)\n",
    "     \n",
    "                                temp_cv = model.historical_forecasts(series=target_series, past_covariates=pc_series,start=val_start, \n",
    "                                                         forecast_horizon=1, stride=1, retrain=True,verbose=False)\n",
    "     \n",
    "                            if(pc_empty == False & fc_empty == True):\n",
    "                            #case when there are no past covariates\n",
    "                                rmodel = RegressionModel(lags=target_lags[:(t+1)], lags_future_covariates = fc_lags[:(f+1)],\n",
    "                                                        lags_past_covariates=pc_lags[:(p+1)],output_chunk_length=1,\n",
    "                                                        model=reg_model)\n",
    "     \n",
    "                                temp_cv = model.historical_forecasts(series=target_series, future_covariates=fc_series,start=val_start,\n",
    "                                                          forecast_horizon=1, stride=1, retrain=True,verbose=False)\n",
    "     \n",
    "\n",
    "                            err_mat[t,p,f,l,nt,s] =  darts.metrics.metrics.rmse(actual_series=target_series, pred_series=temp_cv)   \n",
    "                            print(t,p,f,l,nt,s,np.amin(err_mat))\n",
    "    \n",
    "    #find best hyper parameters.\n",
    "    loc = np.where(err_mat == np.amin(err_mat))\n",
    "    t,p,f,l,nt,s = loc[0][0], loc[1][0], loc[2][0], loc[3][0], loc[4][0], loc[5][0],\n",
    "    print(t,p,f,l,nt,s)\n",
    "\n",
    "    n = (target_df.shape[0]-1)\n",
    "    #screen away variables using all available data, fit model and forecast.\n",
    "    target_series, pc_series, fc_series, pc_empty, fc_empty = variable_screening(p=s_grid[s],n=n,target_df=target_df,pc_df=pc_df,fc_df=fc_df)\n",
    "    reg_model = lgbm(learning_rate=lr_grid[l],n_estimators=ntrees_grid[nt],n_jobs=10,num_leaves=6)\n",
    "\n",
    "    if(pc_empty & fc_empty):\n",
    "        \n",
    "\n",
    "        model = RegressionModel(lags=target_lags[:(t+1)], lags_future_covariates = fc_lags[:(f+1)],\n",
    "                                                        lags_past_covariates=pc_lags[:(p+1)],output_chunk_length=1,\n",
    "                                                        model=reg_model)\n",
    "        model.fit(series=target_series,past_covariates=pc_series,future_covariates=fc_series)     \n",
    "                         \n",
    "\n",
    "    if(pc_empty == True & fc_empty == False):\n",
    "        \n",
    "        model = RegressionModel(lags=target_lags[:(t+1)], lags_future_covariates = fc_lags[:(f+1)],\n",
    "                                                        lags_past_covariates=pc_lags[:(p+1)],output_chunk_length=1,\n",
    "                                                        model=reg_model)\n",
    "    \n",
    "        model.fit(series=target_series, past_covariates=pc_series)\n",
    "     \n",
    "                              \n",
    "     \n",
    "    if(pc_empty == False & fc_empty == True):\n",
    "        \n",
    "        model = RegressionModel(lags=target_lags[:(t+1)], lags_future_covariates = fc_lags[:(f+1)],\n",
    "                                                        lags_past_covariates=pc_lags[:(p+1)],output_chunk_length=1,\n",
    "                                                        model=reg_model)\n",
    "    \n",
    "        model.fit(series=target_series, future_covariates=fc_series)\n",
    "    \n",
    "\n",
    "    #return all the relevant results in a dictionary.\n",
    "\n",
    "    return {'target_date':target_date,'vintage_date':vintage_file[27:42],'forecast':model.predict(n=1).univariate_values()[0],'historical_forecasts_errors':err_mat,\n",
    "            'target lags':target_lags[:(t+1)],\"fc lags\":fc_lags[:(f+1)],\"pc lags\":pc_lags[:(p+1)],\"lr\":lr_grid[l],\"ntrees\":ntrees_grid[nt],\"thres\":s_grid[s]}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run over all vintages\n",
    "results = []\n",
    "\n",
    "for i in range(len(vintage_files)):\n",
    "    print(vintage_files[i])\n",
    "    results.append(forecast_vintage(vintage_files[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('gbdt_results.pickle', 'wb') as handle:\n",
    "#    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gbdt_results.pickle', 'rb') as handle:\n",
    "    results = pickle.load(handle)\n",
    "\n",
    "vintage_names = [i[\"vintage_date\"] for i in results]\n",
    "forecasts = [i[\"forecast\"] for i in results]\n",
    "dt = [results[j]['target_date'] for j in range(len(results))] \n",
    "\n",
    "out = pd.DataFrame({\"ds\":dt,\"vintage_file\":vintage_names,\"Estimate\":forecasts,\"Model\":\"GBDT\"})\n",
    "out.to_csv(\"gbdt_forecasts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store forecasts and targets\n",
    "results.pop()\n",
    "results_dataframe = pd.DataFrame(np.array([j['forecast'] for j in results]).reshape([20,3]),columns=[\"Month1\",\"Month2\",\"Month3\"])\n",
    "\n",
    "dt = [results[j]['target_date'] for j in range(0,60,3)]\n",
    "\n",
    "results_dataframe = results_dataframe.set_index(pd.DatetimeIndex(dt))\n",
    "\n",
    "df = load_data(vintage_files[len(vintage_files)-1])\n",
    "target_df = pd.DataFrame(df[\"y\"])\n",
    "results_dataframe[\"Target\"] = np.array(target_df.iloc[[np.where(df[\"ds\"] == results_dataframe.index[j])[0][0] for j in range(results_dataframe.shape[0])]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute some metrics\n",
    "rmse = []\n",
    "r2_score = []\n",
    "\n",
    "rmse.append(darts.metrics.metrics.rmse(actual_series=darts.TimeSeries.from_series(results_dataframe[\"Target\"]), pred_series=darts.TimeSeries.from_series(results_dataframe[\"Month1\"]))) \n",
    "r2_score.append(darts.metrics.metrics.r2_score(actual_series=darts.TimeSeries.from_series(results_dataframe[\"Target\"]), pred_series=darts.TimeSeries.from_series(results_dataframe[\"Month1\"])))  \n",
    "rmse.append(darts.metrics.metrics.rmse(actual_series=darts.TimeSeries.from_series(results_dataframe[\"Target\"]), pred_series=darts.TimeSeries.from_series(results_dataframe[\"Month2\"])))  \n",
    "r2_score.append(darts.metrics.metrics.r2_score(actual_series=darts.TimeSeries.from_series(results_dataframe[\"Target\"]), pred_series=darts.TimeSeries.from_series(results_dataframe[\"Month2\"])))  \n",
    "rmse.append(darts.metrics.metrics.rmse(actual_series=darts.TimeSeries.from_series(results_dataframe[\"Target\"]), pred_series=darts.TimeSeries.from_series(results_dataframe[\"Month3\"])))  \n",
    "r2_score.append(darts.metrics.metrics.r2_score(actual_series=darts.TimeSeries.from_series(results_dataframe[\"Target\"]), pred_series=darts.TimeSeries.from_series(results_dataframe[\"Month3\"])))  \n",
    "\n",
    "print(rmse)\n",
    "print(r2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "darts.TimeSeries.from_series(results_dataframe[\"Target\"]).plot()\n",
    "darts.TimeSeries.from_series(results_dataframe[\"Month1\"]).plot()\n",
    "darts.TimeSeries.from_series(results_dataframe[\"Month2\"]).plot()\n",
    "darts.TimeSeries.from_series(results_dataframe[\"Month3\"]).plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "darts",
   "language": "python",
   "name": "darts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
