{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US DFM Nowcasting Model\n",
    "\n",
    "This Notebook contains a DFM mixed frequency nowcasting model for US Quarterly GDP in Python, using the latest edition of the [FRED MD database](https://research.stlouisfed.org/econ/mccracken/fred-databases/). The model is adapted from the one set up by [Chad Fulton in 2020](http://www.chadfulton.com/topics/statespace_large_dynamic_factor_models.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief overview\n",
    "\n",
    "The dynamic factor model considered in this notebook can be found in the `DynamicFactorMQ` class, which is a part of the time series analysis component (and in particular the state space models subcomponent) of Statsmodels. It can be accessed as follows:\n",
    "\n",
    "```python\n",
    "import statsmodels.api as sm\n",
    "model = sm.tsa.DynamicFactorMQ(...)\n",
    "```\n",
    "\n",
    "**Data**\n",
    "\n",
    "In this notebook, we'll use 127 monthly variables and 1 quarterly variable (GDP) from the [FRED-MD / FRED-QD dataset](https://research.stlouisfed.org/econ/mccracken/fred-databases/) (McCracken and Ng, 2016).\n",
    "\n",
    "**Statistical model**:\n",
    "\n",
    "The statistical model and the EM-algorithm used for parameter estimation are described in:\n",
    "\n",
    "- Bańbura and Modugno (2014), \"Maximum likelihood estimation of factor models on datasets with arbitrary pattern of missing data.\" ([Working paper](https://core.ac.uk/download/pdf/6684705.pdf), [Published](https://onlinelibrary.wiley.com/doi/full/10.1002/jae.2306?casa_token=tX0xS_49OXcAAAAA%3Aocw-egTRztTVg643NCHRCQUs_OGCPMTS78Qds4gk2nN6ViFjOMZYSDVip-0eeDwQCpvaTOTqjof5_wKI)), and\n",
    "- Bańbura et al. (2011), \"Nowcasting\" ([Working paper](https://core.ac.uk/download/pdf/6518537.pdf), [Handbook chapter](https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780195398649.001.0001/oxfordhb-9780195398649-e-8))\n",
    "\n",
    "As in these papers, the basic specification starts from the typical \"static form\" of the dynamic factor model:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_t & = \\Lambda f_t + \\epsilon_t \\\\\n",
    "f_t & = A_1 f_{t-1} + \\dots + A_p f_{t-p} + u_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The `DynamicFactorMQ` class allows for the generalizations of the model described in the references above, including:\n",
    "\n",
    "- Limiting blocks of factors to only load on certain observed variables\n",
    "- Monthly and quarterly mixed-frequency data, along the lines described by Mariano and Murasawa (2010)\n",
    "- Allowing autocorrelation (via AR(1) specifications) in the idiosyncratic disturbances $\\epsilon_t$\n",
    "- Missing entries in the observed variables $y_t$\n",
    "\n",
    "**Nowcasting, updating forecasts, and computing the \"news\"**\n",
    "\n",
    "By including both monthly and quarterly variables, this model can be used to produce \"nowcasts\" of a quarterly variable before it is released, based on the monthly data for that quarter. For example, the advance estimate for the first quarter GDP is typically released in April, but this model could produce an estimate in March that was based on data through February.\n",
    "\n",
    "Many forecasting and nowcasting exercises are updated frequently, in \"real time\", and it is therefore important that one can easily add new datapoints as they come in. As these new datapoints provide new information, the model's forecast / nowcast will change with new data, and it is also important that one can easily decompose these changes into contributions from each updated series to changes in the forecast.  Both of these steps are supported by all state space models in Statsmodels – including the `DynamicFactorMQ` model – as we show below.\n",
    "\n",
    "**Other resources**\n",
    "\n",
    "The [New York Fed Staff Nowcast](https://www.newyorkfed.org/research/policy/nowcast.html) is an application of this same dynamic factor model and (EM algorithm) estimation method. Although they use a different dataset, and update their results weekly, their underlying framework is the same as that used in this notebook.\n",
    "\n",
    "For more details on the New York Fed Staff Nowcast model and results, see [Bok et al. (2018)](https://www.newyorkfed.org/research/staff_reports/sr830)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "In this notebook, we estimate a dynamic factor model on a large panel of economic data released at a monthly frequency, along with GDP, which is only released at a quarterly frequency. The monthly datasets that we'll be using come from [FRED-MD database](https://research.stlouisfed.org/econ/mccracken/fred-databases/) (McCracken and Ng, 2016), and we will take real GDP from the companion FRED-QD database.\n",
    "\n",
    "**Data vintage**\n",
    "\n",
    "The FRED-MD dataset was launched in January 2015, and vintages are available for each month since then. The FRED-QD dataset was fully launched in May 2018, and monthly vintages are available for each month since then. \n",
    "\n",
    "**Data transformations**\n",
    "\n",
    "The assumptions of the dynamic factor model in this notebook require that the factors and observed variables are stationary. However, this dataset contains raw economic series that clearly violate that assumptions – for example, many of them show distinct trends. As is typical in these exercises, we therefore transform the variables to induce stationarity. In particular, the FRED-MD and FRED-QD datasets include suggested transformations (coded 1-7, which typically apply differences or percentage change transformations) which we apply.\n",
    "\n",
    "The exact details are in the `transform` function, below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(column, transforms):\n",
    "    transformation = transforms[column.name]\n",
    "    # For quarterly data like GDP, we will compute\n",
    "    # annualized percent changes\n",
    "    mult = 4 if column.index.freqstr[0] == 'Q' else 1\n",
    "    \n",
    "    # 1 => No transformation\n",
    "    if transformation == 1:\n",
    "        pass\n",
    "    # 2 => First difference\n",
    "    elif transformation == 2:\n",
    "        column = column.diff()\n",
    "    # 3 => Second difference\n",
    "    elif transformation == 3:\n",
    "        column = column.diff().diff()\n",
    "    # 4 => Log\n",
    "    elif transformation == 4:\n",
    "        column = np.log(column)\n",
    "    # 5 => Log first difference, multiplied by 100\n",
    "    #      (i.e. approximate percent change)\n",
    "    #      with optional multiplier for annualization\n",
    "    elif transformation == 5:\n",
    "        column = np.log(column).diff() * 100 * mult\n",
    "    # 6 => Log second difference, multiplied by 100\n",
    "    #      with optional multiplier for annualization\n",
    "    elif transformation == 6:\n",
    "        column = np.log(column).diff().diff() * 100 * mult\n",
    "    # 7 => Exact percent change, multiplied by 100\n",
    "    #      with optional annualization\n",
    "    elif transformation == 7:\n",
    "        column = ((column / column.shift(1))**mult - 1.0) * 100\n",
    "        \n",
    "    return column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outliers**\n",
    "\n",
    "Following McCracken and Ng (2016), we remove outliers (setting their value to missing), defined as observations that are more than 10 times the interquartile range from the series mean.\n",
    "\n",
    "However, in this exercise we are interested in \"nowcasting\" real GDP growth for 2020Q2, which was greatly affected by economic shutdowns stemming from the COVID-19 pandemic. During the first half of 2020, there are a number of series which include extreme observations, many of which would be excluded by this outlier test. Because these observations are likely to be informative about real GDP in 2020Q2, we only remove outliers for the period 1959-01 through 2019-12.\n",
    "\n",
    "The details of outlier removal are in the `remove_outliers` function, below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(dta):\n",
    "    # Compute the mean and interquartile range\n",
    "    mean = dta.mean()\n",
    "    iqr = dta.quantile([0.25, 0.75]).diff().T.iloc[:, 1]\n",
    "    \n",
    "    # Replace entries that are more than 10 times the IQR\n",
    "    # away from the mean with NaN (denotes a missing entry)\n",
    "    mask = np.abs(dta) > mean + 10 * iqr\n",
    "    treated = dta.copy()\n",
    "    treated[mask] = np.nan\n",
    "\n",
    "    return treated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the data**\n",
    "\n",
    "The `load_fredmd_data` function, below, performs the following actions, once for the FRED-MD dataset and once for the FRED-QD dataset:\n",
    "\n",
    "1. Based on the `vintage` argument, it downloads a particular vintage of these datasets from the base URL https://files.stlouisfed.org/files/htdocs/fred-md into the `orig_[m|q]` variable.\n",
    "2. Extracts the column describing which transformation to apply into the `transform_[m|q]` (and, for the quarterly dataset, also extracts the column describing which factor an earlier paper assigned each variable to).\n",
    "3. Extracts the observation date (from the \"sasdate\" column) and uses it as the index of the dataset.\n",
    "4. Applies the transformations from step (2).\n",
    "5. Removes outliers for the period 1959-01 through 2019-12.\n",
    "\n",
    "Finally, these are collected into an easy-to-use object (the `SimpleNamespace` object) and returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fredmd_data(vintage):\n",
    "    base_url = 'https://files.stlouisfed.org/files/htdocs/fred-md'\n",
    "    \n",
    "    # - FRED-MD --------------------------------------------------------------\n",
    "    # 1. Download data\n",
    "    orig_m = (pd.read_csv(f'{base_url}/monthly/{vintage}.csv')\n",
    "                .dropna(how='all'))\n",
    "    \n",
    "    # 2. Extract transformation information\n",
    "    transform_m = orig_m.iloc[0, 1:]\n",
    "    orig_m = orig_m.iloc[1:]\n",
    "\n",
    "    # 3. Extract the date as an index\n",
    "    orig_m.index = pd.PeriodIndex(orig_m.sasdate.tolist(), freq='M')\n",
    "    orig_m.drop('sasdate', axis=1, inplace=True)\n",
    "\n",
    "    # 4. Apply the transformations\n",
    "    dta_m = orig_m.apply(transform, axis=0,\n",
    "                         transforms=transform_m)\n",
    "\n",
    "    # 5. Remove outliers (but not in 2020)\n",
    "    dta_m.loc[:'2019-12'] = remove_outliers(dta_m.loc[:'2019-12'])\n",
    "\n",
    "    # - FRED-QD --------------------------------------------------------------\n",
    "    # 1. Download data\n",
    "    orig_q = (pd.read_csv(f'{base_url}/quarterly/{vintage}.csv')\n",
    "                .dropna(how='all'))\n",
    "\n",
    "    # 2. Extract factors and transformation information\n",
    "    factors_q = orig_q.iloc[0, 1:]\n",
    "    transform_q = orig_q.iloc[1, 1:]\n",
    "    orig_q = orig_q.iloc[2:]\n",
    "\n",
    "    # 3. Extract the date as an index\n",
    "    orig_q.index = pd.PeriodIndex(orig_q.sasdate.tolist(), freq='Q')\n",
    "    orig_q.drop('sasdate', axis=1, inplace=True)\n",
    "\n",
    "    # 4. Apply the transformations\n",
    "    dta_q = orig_q.apply(transform, axis=0,\n",
    "                          transforms=transform_q)\n",
    "\n",
    "    # 5. Remove outliers (but not in 2020)\n",
    "    dta_q.loc[:'2019Q4'] = remove_outliers(dta_q.loc[:'2019Q4'])\n",
    "    \n",
    "    # - Output datasets ------------------------------------------------------\n",
    "    return types.SimpleNamespace(\n",
    "        orig_m=orig_m, orig_q=orig_q,\n",
    "        dta_m=dta_m, transform_m=transform_m,\n",
    "        dta_q=dta_q, transform_q=transform_q, factors_q=factors_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the current dataset\n",
    "data = load_fredmd_data(\"current\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitions from the Appendix for FRED-MD variables\n",
    "defn_m = pd.read_csv('../data/FRED/fredmd_definitions.csv', encoding_errors='ignore')\n",
    "defn_m.index = defn_m.fred\n",
    "defn_m = defn_m.loc[data.dta_m.columns.intersection(defn_m.fred), :]\n",
    "map_m = defn_m['description'].to_dict()\n",
    "\n",
    "# Definitions from the Appendix for FRED-QD variables\n",
    "defn_q = pd.read_csv('../data/FRED/fredqd_definitions.csv', encoding_errors='ignore')\n",
    "defn_q.index = defn_q.fred\n",
    "defn_q = defn_q.loc[data.dta_q.columns.intersection(defn_q.fred), :]\n",
    "map_q = defn_q['description'].to_dict()\n",
    "\n",
    "# Example of the information in these files:\n",
    "defn_m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting GDP\n",
    "data.orig_q.GDPC1.plot(title = map_q[\"GDPC1\"], figsize = (12, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dta_q.GDPC1.plot(title = \"Real GDP Growth (%)\", figsize = (12, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To aid interpretation of the results, we'll replace the names of our dataset with the \"description\" field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the names of the columns in each monthly and quarterly dataset\n",
    "data.orig_m = data.orig_m[map_m.keys()].rename(columns = map_m)\n",
    "data.dta_m = data.dta_m[map_m.keys()].rename(columns = map_m)\n",
    "data.orig_q = data.orig_q[map_q.keys()].rename(columns = map_q)\n",
    "data.dta_q = data.dta_q[map_q.keys()].rename(columns = map_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data groups**\n",
    "\n",
    "Below, we get the groups for each series from the definition files above, and then show how many of the series that we'll be using fall into each of the groups.\n",
    "\n",
    "We'll also re-order the series by group, to make it easier to interpret the results.\n",
    "\n",
    "Since we're including the quarterly real GDP variable in our analysis, we need to assign it to one of the groups in the monthly dataset. It fits best in the \"Output and income\" group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mapping of variable id to group name, for monthly variables\n",
    "groups = defn_m[['description', 'group']].copy()\n",
    "\n",
    "# Re-order the variables according to the definition CSV file\n",
    "# (which is ordered by group)\n",
    "columns = [name for name in defn_m['description'] if name in data.dta_m.columns]\n",
    "\n",
    "# Add real GDP (our quarterly variable) into the \"Output and Income\" group\n",
    "gdp_description = defn_q.loc['GDPC1', 'description']\n",
    "groups = pd.concat([groups, pd.DataFrame([{'description': gdp_description, 'group': 'Output and Income'}])], \n",
    "                   ignore_index=True)\n",
    "\n",
    "# Display the number of variables in each group\n",
    "(groups.groupby('group', sort=False)\n",
    "       .count()\n",
    "       .rename({'description': '# series in group'}, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Data\n",
    "data.orig_m.to_csv(\"../data/FRED/monthly.csv\")\n",
    "data.dta_m.to_csv(\"../data/FRED/monthly_transformed.csv\")\n",
    "data.orig_q.to_csv(\"../data/FRED/quarterly.csv\")\n",
    "data.dta_q.to_csv(\"../data/FRED/quarterly_transformed.csv\")\n",
    "groups.to_csv(\"../data/FRED/groups.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model specification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Factor specification**\n",
    "\n",
    "Based on screeplots calculated for the overall dataset and within each group, we opt for 2 global factors and 1-2 factors for the different groups, which evolve by (vector) autoregressive processes of order 1-4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the variable => list of factors dictionary\n",
    "factors = {row['description']: ['Global', row['group']]\n",
    "           for ix, row in groups.iterrows()}\n",
    "\n",
    "# Check that we have the desired output for \"Real personal income\"\n",
    "print(factors['Real Personal Income'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Factor multiplicities**\n",
    "\n",
    "The `factor_multiplicities` argument defaults to `1`, but it can be passed a dictionary with keys equal to factor names (from the `factors` argument) and values equal to an integer. Note that the default for each factor is 1, so we only include in this dictionary factors that have multiplicity greater than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_multiplicities = {'Global': 2, \n",
    "                         'Consumption, Orders, and Inventories': 2, \n",
    "                         'Interest and Exchange Rates': 2}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Factor orders**\n",
    "\n",
    "Finally, we need to specify the lag order of the (vector) autoregressions that govern the dynamics of the factors. This is done via the `factor_orders` argument. The `factor_orders` argument also defaults to `1`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_orders = {'Global': 4, \n",
    "                 'Consumption, Orders, and Inventories': 4, \n",
    "                 'Housing': 2,\n",
    "                 'Interest and Exchange Rates': 3,\n",
    "                 'Money and Credit': 2,\n",
    "                 'Output and Income': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the model**\n",
    "\n",
    "Given the factor specification, above, we can finish the model specification and create the model object.\n",
    "\n",
    "The `DynamicFactorMQ` model class has the following primary arguments:\n",
    "\n",
    "1. `endog` and `endog_quarterly`\n",
    "\n",
    "   These arguments are used to pass the observed variables to the model. There are two ways to provide the data:\n",
    "   \n",
    "   1. If you are specifying a monthly / quarterly mixed frequency model, then you would pass the monthly data to `endog` and the quarterly data to the keyword argument `endog_quarterly`. This is what we have done below.\n",
    "   2. If you are specifying any other kind of model, then you simply pass all of your observed data to the `endog` variable and you do not include the `endog_quarterly` argument. In this case, the `endog` data does not need to be monthly - it can be any frequency (or no frequency).\n",
    "\n",
    "\n",
    "2. `factors`, `factor_orders`, and `factor_multiplicities`\n",
    "\n",
    "   These arguments were described above.\n",
    "\n",
    "\n",
    "3. `idiosyncratic_ar1`\n",
    "\n",
    "   As noted in the \"Brief Overview\" section, above, the `DynamicFactorMQ` model allows the idiosyncratic disturbance terms to be modeled as independent AR(1) processes or as iid variables. The default is `idiosyncratic_ar1=True`, which can be useful in modeling some of the idiosyncratic serial correlation, for example for forecasting.\n",
    "\n",
    "\n",
    "4. `standardize`\n",
    "\n",
    "   Although we earlier transformed all of the variables to be stationary, they will still fluctuate around different means and they may have very different scales. This can make it difficult to fit the model. In most applications, therefore, the variables are standardized by subtracting the mean and dividing by the standard deviation. This is the default, so we do not set this argument below.\n",
    "\n",
    "   It is recommended for users to use this argument rather than standardizing the variables themselves. This is because if the `standardize` argument is used, then the model will automatically reverse the standardization for all post-estimation results like prediction, forecasting, and computation of the impacts of the \"news\". This means that these results can be directly compared to the input data.\n",
    "   \n",
    "**Note**: to generate the best nowcasts for GDP growth in 2020Q2, we will restrict the sample to start in 2000-01 rather than in 1960 (for example, to guard against the possibility of structural changes in underlying parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the baseline monthly and quarterly datasets\n",
    "start = '2000'\n",
    "endog_m = data.dta_m.loc[start:, :]\n",
    "gdp_description = defn_q.loc['GDPC1', 'description']\n",
    "endog_q = data.dta_q.loc[start:, [gdp_description]]\n",
    "\n",
    "# Construct the dynamic factor model\n",
    "model = sm.tsa.DynamicFactorMQ(\n",
    "    endog_m, endog_quarterly=endog_q,\n",
    "    factors=factors, factor_orders=factor_orders,\n",
    "    factor_multiplicities=factor_multiplicities)\n",
    "\n",
    "# Another model without separate blocks but 9 global factors\n",
    "model_global = sm.tsa.DynamicFactorMQ(\n",
    "    endog_m, endog_quarterly=endog_q,\n",
    "    factors = 1, # {x: 'Global' for x in list(groups.description)},\n",
    "    factor_orders = 4, # {'Global': 4},\n",
    "    factor_multiplicities= 9) #{'Global': 9})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()\n",
    "model_global.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit blocked model\n",
    "results = model.fit(disp=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit global model\n",
    "results_global = model_global.fit(disp=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_global.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimated factors\n",
    "\n",
    "In addition to the estimates of the parameters, the `results` object contains the estimates of the latent factors. These are most conveniently accessed through the `factors` attribute. This attribute in turn contains four sub-attributes:\n",
    "\n",
    "- `smoothed`: estimates of the factors, conditional on the full dataset (also called \"smoothed\" or \"two-sided\" estimates)\n",
    "- `smoothed_cov`: covariance matrix of the factor estimates, conditional on the full dataset\n",
    "- `filtered`: estimates of the factors, where the estimate at time $t$ only uses information through time $t$ (also called \"filtered\" or \"one-sided\" estimates\n",
    "- `filtered_cov`: covariance matrix of the factor estimates, where the estimate at time $t$ only uses information through time $t$\n",
    "\n",
    "As an example, in the next cell we plot three of the smoothed factors and 95% confidence intervals.\n",
    "\n",
    "**Note**: The estimated factors are not identified without additional assumptions that this model does not impose (see for example Bańbura and Modugno, 2014, for details). As a result, it can be difficult to interpet the factors themselves. (Despite this, the space spanned by the factors *is* identified, so that forecasting and nowcasting exercises, like those we discuss later, are unambiguous).\n",
    "\n",
    "For example, in the plot\n",
    "below, the \"Global.1\" factor increases markedly in 2009, following the global financial crisis. However, many of the factor loadings in the summary above are negative – for example, this is true of the output, consumption, and income series. Therefore the increase in the \"Global.1\" factor during this period actually implies a strong *decrease* in output, consumption, and income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x for x in dir(results) if not x.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_factors(results, factor_names):\n",
    "    # Get estimates of the global and labor market factors,\n",
    "    # conditional on the full dataset (\"smoothed\")\n",
    "    mean = results.factors.smoothed[factor_names]\n",
    "\n",
    "    # Compute 95% confidence intervals\n",
    "    from scipy.stats import norm\n",
    "    std = pd.concat([results.factors.smoothed_cov.loc[name, name]\n",
    "                    for name in factor_names], axis=1)\n",
    "    crit = norm.ppf(1 - 0.05 / 2)\n",
    "    lower = mean - crit * std\n",
    "    upper = mean + crit * std\n",
    "\n",
    "    with sns.color_palette('deep'):\n",
    "        fig, ax = plt.subplots(figsize=(14, 6))\n",
    "        mean.plot(ax=ax)\n",
    "        \n",
    "        for name in factor_names:\n",
    "            ax.fill_between(mean.index, lower[name], upper[name], alpha=0.3)\n",
    "        \n",
    "        ax.set(title='Estimated factors: smoothed estimates and 95% confidence intervals')\n",
    "        fig.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_names = results.factors.smoothed.columns.to_list() # ['Global.1', 'Global.2', 'Labor Market']\n",
    "plot_factors(results, factor_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_factors(results_global, results_global.factors.smoothed.columns.to_list())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanatory power of the factors\n",
    "\n",
    "One way to examine how the factors relate to the observed variables is to compute the explanatory power that each factor has for each variable, by regressing each variable on a constant plus one or more of the smoothed factor estimates and storing the resulting $R^2$, or \"coefficient of determination\", value.\n",
    "\n",
    "**Computing $R^2$**\n",
    "\n",
    "The `get_coefficients_of_determination` method in the results object has three options for the `method` argument:\n",
    "\n",
    "- `method='individual'` retrieves the $R^2$ value for each observed variable regressed on each individual factor (plus a constant term)\n",
    "- `method='joint'` retrieves the $R^2$ value for each observed variable regressed on all factors that the variable loads on\n",
    "- `method='cumulative'` retrieves the $R^2$ value for each observed variable regressed on an expanding set of factors. The expanding set begins with the $R^2$ from a regression of each variable on the first factor that the variable loads on (as it appears in, for example, the summary tables above) plus a constant. For the next factor in the list, the $R^2$ is computed by a regression on the first two factors (assuming that a given variable loads on both factors).\n",
    "\n",
    "**Example:** top 10 variables explained by the global factors\n",
    "\n",
    "Below, we compute according to the `method='individual'` approach, and then show the top 10 observed variables that are explained (individually) by each of the two global factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsquared = results.get_coefficients_of_determination(method='individual')\n",
    "factor_names = ['Global.1', 'Global.2']\n",
    "\n",
    "top_ten = []\n",
    "for factor_name in factor_names:\n",
    "    top_factor = (rsquared[factor_name].sort_values(ascending=False)\n",
    "                                       .iloc[:10].round(2).reset_index())\n",
    "    top_factor.columns = pd.MultiIndex.from_product([\n",
    "        [f'Top ten variables explained by {factor_name}'],\n",
    "        ['Variable', r'$R^2$']])\n",
    "    top_ten.append(top_factor)\n",
    "pd.concat(top_ten, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting $R^2$**\n",
    "\n",
    "When there are a large number of observed variables, it is often easier to plot the $R^2$ values for each variable. This can be done using the `plot_coefficients_of_determination` method in the results object. It accepts the same `method` arguments as the `get_coefficients_of_determination` method, above.\n",
    "\n",
    "Below, we plot the $R^2$ values from the \"individual\" regressions, for each factor. Because there are so many variables, this graphical tool is best for identifying trends overall and within groups, and we do not display the names of the variables on the x-axis label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.color_palette('deep'):\n",
    "    fig = results.plot_coefficients_of_determination(method='individual', figsize=(14, 12))\n",
    "    fig.suptitle(r'$R^2$ - regression on individual factors', fontsize=14, fontweight=600)\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.color_palette('deep'):\n",
    "    fig = results_global.plot_coefficients_of_determination(method='individual', figsize=(14, 12))\n",
    "    fig.suptitle(r'$R^2$ - regression on individual factors', fontsize=14, fontweight=600)\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we might look at the overall explanatory value to a given variable of all factors that the variable loads on. To do that, we can use the same function but with `method='joint'` .\n",
    "\n",
    "To make it easier to identify patterns, we add in shading and labels to identify the different groups of variables, as well as our only quarterly variable, GDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_r2_joint(results):\n",
    "    group_counts = defn_m[['description', 'group']]\n",
    "    group_counts = group_counts[group_counts['description'].isin(data.dta_m.columns)]\n",
    "    group_counts = group_counts.groupby('group', sort=False).count()['description'].cumsum()\n",
    "\n",
    "    with sns.color_palette('deep'):\n",
    "        fig = results.plot_coefficients_of_determination(method='joint', figsize=(14, 3));\n",
    "\n",
    "        # Add in group labels\n",
    "        ax = fig.axes[0]\n",
    "        ax.set_ylim(0, 1.2)\n",
    "        for i in np.arange(1, len(group_counts), 2):\n",
    "            start = 0 if i == 0 else group_counts[i - 1]\n",
    "            end = group_counts[i] + 1\n",
    "            ax.fill_between(np.arange(start, end) - 0.6, 0, 1.2, color='k', alpha=0.1)\n",
    "        for i in range(len(group_counts)):\n",
    "            start = 0 if i == 0 else group_counts[i - 1]\n",
    "            end = group_counts[i]\n",
    "            n = end - start\n",
    "            text = group_counts.index[i]\n",
    "            if len(text) > n:\n",
    "                text = text[:n - 3] + '...'\n",
    "\n",
    "            ax.annotate(text, (start + n / 2, 1.1), ha='center')\n",
    "\n",
    "        # Add label for GDP\n",
    "        ax.set_xlim(-1.5, model.k_endog + 0.5)\n",
    "        ax.annotate('GDP', (model.k_endog - 1.1, 1.05), ha='left', rotation=90)\n",
    "\n",
    "        fig.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_r2_joint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_r2_joint(results_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting\n",
    "\n",
    "One of the benefits of these models is that we can use the dynamics of the factors to produce forecasts of any of the observed variables. This is straightforward here, using the `forecast` or `get_forecast` results methods. These take a single argument, which must be either:\n",
    "\n",
    "- an integer, specifying the number of steps ahead to forecast\n",
    "- a date, specifying the date of the final forecast to make\n",
    "\n",
    "The `forecast` method only produces a series of point forecasts for all of the observed variables, while the `get_forecast` method returns a new forecast results object, that can also be used to compute confidence intervals. \n",
    "\n",
    "**Note**: these forecasts are in the same scale as the variables passed to the `DynamicFactorMQ` constructor, even if `standardize=True` has been used.\n",
    "\n",
    "Below is an example of the `forecast` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create point forecasts, 3 steps ahead\n",
    "point_forecasts = results.forecast(steps=3)\n",
    "\n",
    "# Print the forecasts for the first 5 observed variables\n",
    "print(point_forecasts.T.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to `forecast` and `get_forecast`, there are two more general methods, `predict` and `get_prediction` that allow for both of in-sample prediction and out-of-sample forecasting. Instead of a `steps` argument, they take `start` and `end` arguments, which can be either in-sample dates or out-of-sample dates.\n",
    "\n",
    "Below, we give an example of using `get_prediction` to show in-sample predictions and out-of-sample forecasts for some spreads between Treasury securities and the Federal Funds Rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create forecasts results objects, through the end of 2024\n",
    "prediction_results = results.get_prediction(start='2000', end='2024')\n",
    "prediction_results_global = results_global.get_prediction(start='2000', end='2024')\n",
    "\n",
    "def plot_prediction(prediction_results, variables, start = '2000', end = '2024'):\n",
    "\n",
    "    # The `predicted_mean` attribute gives the same\n",
    "    # point forecasts that would have been returned from\n",
    "    # using the `predict` or `forecast` methods.\n",
    "    point_predictions = prediction_results.predicted_mean[variables]\n",
    "\n",
    "    # We can use the `conf_int` method to get confidence\n",
    "    # intervals; here, the 95% confidence interval\n",
    "    ci = prediction_results.conf_int(alpha=0.05)\n",
    "    lower = ci[[f'lower {name}' for name in variables]]\n",
    "    upper = ci[[f'upper {name}' for name in variables]]\n",
    "\n",
    "    latest = str(data.orig_m.index[-1])\n",
    "\n",
    "    # Plot the forecasts and confidence intervals\n",
    "    with sns.color_palette('deep'):\n",
    "        fig, ax = plt.subplots(figsize=(14, 4))\n",
    "\n",
    "        # Plot the in-sample predictions\n",
    "        point_predictions.loc[:latest].plot(ax=ax)\n",
    "\n",
    "        # Plot the out-of-sample forecasts\n",
    "        point_predictions.loc[latest:].plot(ax=ax, linestyle='--',\n",
    "                                            color=['C0', 'C1', 'C2'],\n",
    "                                            legend=False)\n",
    "\n",
    "        # Confidence intervals\n",
    "        for name in variables:\n",
    "            ax.fill_between(ci.index,\n",
    "                            lower[f'lower {name}'],\n",
    "                            upper[f'upper {name}'], alpha=0.1)\n",
    "            \n",
    "        # Forecast period, set title\n",
    "        ylim = ax.get_ylim()\n",
    "        ax.vlines(latest, ylim[0], ylim[1], linewidth=1)\n",
    "        ax.annotate(r' Forecast $\\rightarrow$', (latest, -1.7))\n",
    "        ax.set(title='In-sample predictions and out-of-sample forecasts, with 95% confidence intervals', ylim=ylim)\n",
    "        \n",
    "        fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['1-Year Treasury C Minus FEDFUNDS',\n",
    "             '5-Year Treasury C Minus FEDFUNDS',\n",
    "             '10-Year Treasury C Minus FEDFUNDS']\n",
    "             \n",
    "plot_prediction(prediction_results_global, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['Real Gross Domestic Product, 3 Decimal (Billions of Chained 2012 Dollars)'] \n",
    "             # 'Real Personal Income', 'Real personal consumption expenditures']\n",
    "\n",
    "plot_prediction(prediction_results_global, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forecasting example\n",
    "\n",
    "The variables that we showed in the forecasts above were not transformed from their original values. As a result, the predictions were already interpretable as spreads. For the other observed variables that were transformed prior to construting the model, our forecasts will be in the transformed scale.\n",
    "\n",
    "For example, although the original data in the FRED-MD/QD datasets for Real GDP is in \"Billions of Chained 2012 Dollars\", this variable was transformed to the annualized quarterly growth rate (percent change) for inclusion in the model. Similarly, the Civilian Unemployment Rate was originally in \"Percent\", but it was transformed into the 1-month change (first difference) for inclusion in the model.\n",
    "\n",
    "Because the transformed data was provided to the model, the prediction and forecasting methods will produce predictions and forecasts in the transformed space. (Reminder: the transformation step, which we did prior to constructing the model, is different from the standardization step, which the model handles automatically, and which we do not need to manually reverse).\n",
    "\n",
    "Below, we compute and plot the forecasts directly from the model associated with real GDP and the unemployment rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in dir(prediction_results) if not x.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the titles of the variables as they appear in the dataset\n",
    "unemp_description = 'Civilian Unemployment Rate'\n",
    "gdp_description = 'Real Gross Domestic Product, 3 Decimal (Billions of Chained 2012 Dollars)'\n",
    "\n",
    "latest = str(data.orig_m.index[-1])\n",
    "\n",
    "# Compute the point forecasts\n",
    "point_fcst = prediction_results.predicted_mean.loc['2023-01':, :]\n",
    "fcast_m = point_fcst[unemp_description]\n",
    "fcast_q = point_fcst[gdp_description].resample('Q').last()\n",
    "\n",
    "# For more convenient plotting, combine the observed data with the forecasts\n",
    "plot_m = pd.concat([data.dta_m.loc['2000':, unemp_description], fcast_m])\n",
    "plot_q = pd.concat([data.dta_q.loc['2000':, gdp_description], fcast_q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.color_palette('deep'):\n",
    "    fig, axes = plt.subplots(2, figsize=(14, 7))\n",
    "\n",
    "    # Plot real GDP growth, data and forecasts\n",
    "    plot_q.plot(ax=axes[0])\n",
    "    axes[0].set(title='Real Gross Domestic Product (transformed: annualized growth rate)')\n",
    "    axes[0].hlines(0, plot_q.index[0], plot_q.index[-1], linewidth=1)\n",
    "\n",
    "    # Plot the change in the unemployment rate, data and forecasts\n",
    "    plot_m.plot(ax=axes[1])\n",
    "    axes[1].set(title='Civilian Unemployment Rate (transformed: change)')\n",
    "    axes[1].hlines(0, plot_m.index[0], plot_m.index[-1], linewidth=1)\n",
    "\n",
    "    # Show the forecast period in each graph\n",
    "    for i in range(2):\n",
    "        ylim = axes[i].get_ylim()\n",
    "        axes[i].fill_between(plot_q.loc['2023-01':].index,\n",
    "                             ylim[0], ylim[1], alpha=0.1, color='C0')\n",
    "        axes[i].annotate(r' Forecast $\\rightarrow$',\n",
    "                         ('2022-08', ylim[0] + 0.1 * ylim[1]))\n",
    "        axes[i].set_ylim(ylim)\n",
    "\n",
    "    # Title\n",
    "    fig.suptitle('Data and forecasts (February 2020 vintage), transformed scale',\n",
    "                 fontsize=14, fontweight=600)\n",
    "\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse the transformations\n",
    "\n",
    "# For real GDP, we take the level in 2000Q1 from the original data,\n",
    "# and then apply the growth rates to compute the remaining levels\n",
    "plot_q_orig = (plot_q / 100 + 1)**0.25\n",
    "plot_q_orig.loc['2000Q1'] = data.orig_q.loc['2000Q1', gdp_description]\n",
    "plot_q_orig = plot_q_orig.cumprod()\n",
    "\n",
    "# For the unemployment rate, we take the level in 2000-01 from\n",
    "# the original data, and then we apply the changes to compute the\n",
    "# remaining levels\n",
    "plot_m_orig = plot_m.copy()\n",
    "plot_m_orig.loc['2000-01'] = data.orig_m.loc['2000-01', unemp_description]\n",
    "plot_m_orig = plot_m_orig.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.color_palette('deep'):\n",
    "    fig, axes = plt.subplots(2, figsize=(14, 7))\n",
    "\n",
    "    # Plot real GDP, data and forecasts\n",
    "    plot_q_orig.plot(ax=axes[0])\n",
    "    axes[0].set(title=('Real Gross Domestic Product'\n",
    "                       ' (original scale: Billions of Chained 2012 Dollars)'))\n",
    "\n",
    "    # Plot the unemployment rate, data and forecasts\n",
    "    plot_m_orig.plot(ax=axes[1])\n",
    "    axes[1].set(title='Civilian Unemployment Rate (original scale: Percent)')\n",
    "\n",
    "    # Show the forecast period in each graph\n",
    "    for i in range(2):\n",
    "        ylim = axes[i].get_ylim()\n",
    "        axes[i].fill_between(plot_q.loc['2023-01':].index,\n",
    "                             ylim[0], ylim[1], alpha=0.1, color='C0')\n",
    "        axes[i].annotate(r' Forecast $\\rightarrow$',\n",
    "                         ('2022-08', ylim[0] + 0.5 * (ylim[1] - ylim[0])))\n",
    "        axes[i].set_ylim(ylim)\n",
    "\n",
    "    # Title\n",
    "    fig.suptitle('Data and forecasts (February 2020 vintage), original scale',\n",
    "                 fontsize=14, fontweight=600)\n",
    "\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95]);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eda7e54fe21129b67f77862937907ee926f057597a3e2fa1e18ac955e40912b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
