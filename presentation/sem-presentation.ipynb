{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Uncovering Machine Learning's Potential in Nowcasting GDP\n",
        "format:\n",
        "  revealjs:\n",
        "    theme: slides.scss\n",
        "    slide-number: true\n",
        "    transition: fade\n",
        "    background-transition: fade\n",
        "code-link: true\n",
        "code-fold: true\n",
        "execute:\n",
        "  echo: true\n",
        "  freeze: auto\n",
        "bibliography: biblio.bib\n",
        "---"
      ],
      "id": "a7aa187e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction\n",
        "\n",
        "## Collaborators\n",
        "\n",
        "We have a team of people that consists of statisticians, econometricians and data scientists.\n",
        "\n",
        "-   **Dawie van Lill** üó£Ô∏è\n",
        "-   Fran√ßois Kamper\n",
        "-   Sebastian Krantz\n",
        "\n",
        "::: aside\n",
        "üìß dvanlill\\@sun.ac.za\n",
        ":::\n",
        "\n",
        "## Research question(s)\n",
        "\n",
        "There are two parts to our research question. One part relates to performance, the other to ease of use.\n",
        "\n",
        "<br>\n",
        "\n",
        "> Do machine learning (and deep learning) methods contribute to forecasting performance over and above the traditional forecasting models? How easy are these methods to implement?\n",
        "\n",
        "## Managing expectations\n",
        "\n",
        "<br>\n",
        "\n",
        "This is **not** a technical talk on the methods used.\n",
        "\n",
        "The talk is more about our results and the way that our experience translates to practitioners.\n",
        "\n",
        "<br>\n",
        "\n",
        "::: callout-important\n",
        "## Please reach out\n",
        "\n",
        "Contact us after the presentation to discuss model details. We would love to know how we can improve our models!\n",
        ":::\n",
        "\n",
        "## Contribution\n",
        "\n",
        "Naive forecasting benchmark versus DFM, ML and DL\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"50%\" style=\"text-align: center;\"}\n",
        "**DFM and ML models**\n",
        "\n",
        "::: goal\n",
        "1.  DFM (M)\n",
        "2.  SVR (M)\n",
        "3.  RF (M) **\n",
        "4.  GBM (M) **\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\" style=\"text-align: center;\"}\n",
        "**MLP-based models**\n",
        "\n",
        "::: goal\n",
        "5.  MLP (M)\n",
        "6.  N-BEATS (U & M)\n",
        "7.  N-HiTS (M)\n",
        "\n",
        "::: aside\n",
        "U = Univariate, M = Multivariate, MLP = Multilayer Perceptron\n",
        ":::\n",
        ":::\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Contribution\n",
        "\n",
        "Naive forecasting benchmark versus DFM, ML and DL\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"50%\" style=\"text-align: center;\"}\n",
        "**RNN-based models**\n",
        "\n",
        "::: goal\n",
        "8.  RNN (M)\n",
        "9.  RNN-LSTM (M)\n",
        "10. RNN-GRU (M)\n",
        "11. TCN (M)\n",
        "12. DilatedRNN (M)\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\" style=\"text-align: center;\"}\n",
        "**Transformer models**\n",
        "\n",
        "::: goal\n",
        "13. Temporal Fusion Transformer (U)\n",
        "14. Informer (M)\n",
        "15. Autoformer (M)\n",
        "\n",
        "::: aside\n",
        "RNN = Recurrent neural network\n",
        ":::\n",
        ":::\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Useful libraries\n",
        "\n",
        "<br>\n",
        "\n",
        "![](02_figures/ray_tune.png){.absolute top=\"100\" left=\"50\" width=\"250\" height=\"200\"}\n",
        "\n",
        "![](02_figures/nixtla_new.png){.absolute top=\"75\" right=\"150\" width=\"350\" height=\"350\"}\n",
        "\n",
        "![](02_figures/Darts-Time-Series-Made-Easy-in-Python-100-694x392.jpg){.absolute bottom=\"20\" right=\"300\" width=\"500\" height=\"300\"}\n",
        "\n",
        "# Data\n",
        "\n",
        "## Data\n",
        "\n",
        "<br>\n",
        "\n",
        "The **target variable** represents the seasonally adjusted, annualized quarter-on-quarter GDP growth rate from 1959Q2 to 2023Q2\n",
        "\n",
        "<br>\n",
        "\n",
        "Data for quarterly and monthly macroeconomic variables are gathered from the FRED-QD and FRED-MD databases, with suggested transformations applied.\n",
        "\n",
        "\n",
        "## Data preparation\n",
        "\n",
        "<br>\n",
        "\n",
        "Monthly vintage data is available, while GDP data is at a quarterly frequency, presenting a mixed frequency issue\n",
        "\n",
        "<br>\n",
        "\n",
        "To resolve this problem, we **block** the monthly data. \n",
        "\n",
        " - This entails creating new variables from the values of the first, second, and third month of each quarter.\n",
        "\n",
        "# Dynamic factor models\n",
        "\n",
        "## Dynamic factor models {.smaller}\n",
        "\n",
        "<br>\n",
        "\n",
        "We employ a Mixed Frequency DFM per @banbura2014maximum with 9 factors evolving in a global VAR(4)\n",
        "\n",
        "<br>\n",
        "\n",
        "We also use a blocked factor structure as in @bok2018macroeconomic with 13 factors in 9 blocks, covering various economic sectors. The blocks and number of factors are presented below\n",
        "\n",
        "::: goal\n",
        "global (2), output and income (1), labor market (1), consumption, orders, and inventories (2), housing (1), money and credit (2), interest and exchange rates (2), prices (1), and stock market (1).\n",
        ":::\n",
        "\n",
        "## Dynamic factor models {.smaller}\n",
        "\n",
        "<br> \n",
        "\n",
        "Finally, we estimate bridge-equation models with monthly DFMs using two methods:\n",
        "\n",
        "1. Aggregating monthly factors to nowcast GDP via a linear model\n",
        "2. Distributing monthly factors into various quarterly variables and applying LASSO for nowcasting\n",
        "\n",
        "<br>\n",
        "\n",
        "These bridge models allow for both **global** and *blocked* factor structures.\n",
        "\n",
        "<br>\n",
        "\n",
        "Our goal in using both mixed-frequency and bridge equation models is to assess potential gains from mixed-frequency in DFM estimation.\n",
        "\n",
        "##\n"
      ],
      "id": "9803c307"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-dfm\n",
        "#| fig-cap: DFM models\n",
        "\n",
        "\n",
        "# Load your data\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.dates import YearLocator\n",
        "from matplotlib.ticker import MultipleLocator, AutoMinorLocator\n",
        "data = pd.read_csv('../presentation/01_data/ALL_DFM_results_long.csv')\n",
        "\n",
        "# Convert 'ds' to datetime\n",
        "data['ds'] = pd.to_datetime(data['ds'])\n",
        "\n",
        "# Define the models to plot\n",
        "models_to_plot = ['global', 'blocked', 'lm_blocked',\n",
        "                  'lm_global', 'lasso_min_blocked', 'lasso_min_global']\n",
        "\n",
        "# Filter data for selected models\n",
        "filtered_data = data[data['model'].isin(models_to_plot)]\n",
        "\n",
        "# Group by model and select every third row\n",
        "grouped = filtered_data.groupby('model')\n",
        "\n",
        "selected_data = pd.concat([group.iloc[::3] for _, group in grouped])\n",
        "\n",
        "# Map models to colors\n",
        "model_color_dict = {\n",
        "    'global': 'red',\n",
        "    'blocked': 'green',\n",
        "    'lm_global': 'blue',\n",
        "    'lm_blocked': 'brown',\n",
        "    'lasso_min_blocked': 'orange',\n",
        "    'lasso_min_global': 'purple',\n",
        "}\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Change this line to set x ticks every year\n",
        "ax.xaxis.set_major_locator(YearLocator())\n",
        "\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "\n",
        "ax.tick_params(which=\"major\", width=1.0)\n",
        "ax.tick_params(which=\"major\", length=10)\n",
        "ax.tick_params(which=\"minor\", width=1.0, labelsize=10)\n",
        "ax.tick_params(which=\"minor\", length=5, labelsize=10, labelcolor=\"0.25\")\n",
        "\n",
        "ax.set_ylabel(\"Estimate\", weight=\"medium\")\n",
        "ax.set_xlabel(\"Date\", weight=\"medium\")\n",
        "\n",
        "for model in selected_data['model'].unique():\n",
        "    subset = selected_data[selected_data['model'] == model]\n",
        "    ax.scatter(subset['ds'], subset['value'], s=10, color=model_color_dict[model],\n",
        "               edgecolor=model_color_dict[model], linewidth=1, zorder=-20, alpha=0.3)\n",
        "    ax.plot(subset['ds'], subset['value'],\n",
        "            c=model_color_dict[model], linewidth=1, alpha=0.5, label=model)\n",
        "\n",
        "# Display the legend\n",
        "ax.legend(frameon=False)\n",
        "\n",
        "plt.show()"
      ],
      "id": "fig-dfm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Machine learning models\n",
        "\n",
        "## Support vector regression\n",
        "\n",
        "<center>![](02_figures/svr.png){width=\"75%\"}</center>\n",
        "\n",
        "## Support vector regression {.smaller}\n",
        "\n",
        "<br>\n",
        "In support vector regression (SVR) we fit a function from a reproducing kernel Hilbert space (RKHS) to the data \n",
        "via the epsilon ($\\epsilon$) insensitive loss function. \n",
        "\n",
        "<br>\n",
        "The fit is regularized via the so-called cost parameter, to be selected via temporal cross-validation, and we set $\\epsilon = 1$ throughout.\n",
        "\n",
        "<br> \n",
        "We consider forming RKHS via the **linear, radial basis** and **sigmoid** kernels. The latter two kernels require \n",
        "specification of a hyper-parameter to be selected via temporal cross-validation. \n",
        "\n",
        "## Support vector regression {.smaller}\n",
        "\n",
        "<br>\n",
        "We found that including all the available predictive features in the fitting process to adversely effect results.To circumvent this, we applied a simple screening process to data from the training period only.\n",
        "\n",
        "<br>\n",
        "We only include variables with absolute correlation with the target (over the training period) exceeding a specified threshold. The threshold is chosen using temporal cross-validation.\n",
        "\n",
        "\n",
        "\n",
        "## \n"
      ],
      "id": "3a3b2554"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-svr\n",
        "#| fig-cap: SVR models\n",
        "\n",
        "\n",
        "# Load your data\n",
        "data = pd.read_csv('../presentation/01_data/tidy_svr.csv')\n",
        "\n",
        "# Convert 'ds' to datetime\n",
        "data['ds'] = pd.to_datetime(data['ds'])\n",
        "\n",
        "# Define the models to plot\n",
        "models_to_plot = ['SVR_LINEAR', 'SVR_RBF',\n",
        "                  'SVR_SIGMOID']\n",
        "\n",
        "# Filter data for selected models\n",
        "filtered_data = data[data['Model'].isin(models_to_plot)]\n",
        "\n",
        "# Group by model and select every third row\n",
        "grouped = filtered_data.groupby('Model')\n",
        "\n",
        "selected_data = pd.concat([group.iloc[::3] for _, group in grouped])\n",
        "\n",
        "# Map models to colors\n",
        "model_color_dict = {\n",
        "    'SVR_LINEAR': 'red',\n",
        "    'SVR_RBF': 'green',\n",
        "    'SVR_SIGMOID': 'blue',\n",
        "}\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Change this line to set x ticks every year\n",
        "ax.xaxis.set_major_locator(YearLocator())\n",
        "\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "\n",
        "ax.tick_params(which=\"major\", width=1.0)\n",
        "ax.tick_params(which=\"major\", length=10)\n",
        "ax.tick_params(which=\"minor\", width=1.0, labelsize=10)\n",
        "ax.tick_params(which=\"minor\", length=5, labelsize=10, labelcolor=\"0.25\")\n",
        "\n",
        "ax.set_ylabel(\"Estimate\", weight=\"medium\")\n",
        "ax.set_xlabel(\"Date\", weight=\"medium\")\n",
        "\n",
        "for model in selected_data['Model'].unique():\n",
        "    subset = selected_data[selected_data['Model'] == model]\n",
        "    ax.scatter(subset['ds'], subset['Estimate'], s=10, color=model_color_dict[model],\n",
        "               edgecolor=model_color_dict[model], linewidth=1, zorder=-20, alpha=0.3)\n",
        "    ax.plot(subset['ds'], subset['Estimate'],\n",
        "            c=model_color_dict[model], linewidth=1, alpha=0.5, label=model)\n",
        "\n",
        "# Display the legend\n",
        "ax.legend(frameon=False)\n",
        "\n",
        "plt.show()"
      ],
      "id": "fig-svr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MLP-based models\n",
        "\n",
        "<center>![](02_figures/mlp_model.png){width=\"70%\"}</center>\n",
        "\n",
        "## MLP-based models {.smaller}\n",
        "\n",
        "<br>\n",
        "\n",
        "We explore three types of MLP-based models:\n",
        "\n",
        "1. Standard MLP\n",
        "2. N-BEATS and N-BEATSx\n",
        "3. N-HiTS\n",
        "\n",
        "<br>\n",
        "\n",
        "Both N-BEATS and N-HiTS utilize a learnable architecture to directly capture the historical data's backward and forward-looking components.\n",
        "\n",
        "\n",
        "## \n"
      ],
      "id": "bc19e231"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-mlp\n",
        "#| fig-cap: MLP models\n",
        "\n",
        "\n",
        "# Load your data\n",
        "data = pd.read_csv('../presentation/01_data/tidy_results.csv')\n",
        "\n",
        "# Convert 'ds' to datetime\n",
        "data['ds'] = pd.to_datetime(data['ds'])\n",
        "\n",
        "# Define the models to plot\n",
        "models_to_plot = ['AutoMLP', 'AutoNHITS',\n",
        "                  'AutoNBEATS', 'AutoNBEATSx']\n",
        "\n",
        "# Filter data for selected models\n",
        "filtered_data = data[data['Model'].isin(models_to_plot)]\n",
        "\n",
        "# Group by model and select every third row\n",
        "grouped = filtered_data.groupby('Model')\n",
        "\n",
        "selected_data = pd.concat([group.iloc[::3] for _, group in grouped])\n",
        "\n",
        "# Map models to colors\n",
        "model_color_dict = {\n",
        "    'AutoMLP': 'red',\n",
        "    'AutoNHITS': 'green',\n",
        "    'AutoNBEATS': 'blue',\n",
        "    'AutoNBEATSx': 'cyan'\n",
        "}\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Change this line to set x ticks every year\n",
        "ax.xaxis.set_major_locator(YearLocator())\n",
        "\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "\n",
        "ax.tick_params(which=\"major\", width=1.0)\n",
        "ax.tick_params(which=\"major\", length=10)\n",
        "ax.tick_params(which=\"minor\", width=1.0, labelsize=10)\n",
        "ax.tick_params(which=\"minor\", length=5, labelsize=10, labelcolor=\"0.25\")\n",
        "\n",
        "ax.set_ylabel(\"Estimate\", weight=\"medium\")\n",
        "ax.set_xlabel(\"Date\", weight=\"medium\")\n",
        "\n",
        "for model in selected_data['Model'].unique():\n",
        "    subset = selected_data[selected_data['Model'] == model]\n",
        "    ax.scatter(subset['ds'], subset['Estimate'], s=10, color=model_color_dict[model],\n",
        "               edgecolor=model_color_dict[model], linewidth=1, zorder=-20, alpha=0.3)\n",
        "    ax.plot(subset['ds'], subset['Estimate'],\n",
        "            c=model_color_dict[model], linewidth=1, alpha=0.5, label=model)\n",
        "\n",
        "# Display the legend\n",
        "ax.legend(frameon=False)\n",
        "\n",
        "plt.show()"
      ],
      "id": "fig-mlp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RNN-based models\n",
        "\n",
        "<center>![](02_figures/rnn_model.png){width=\"70%\"}</center>\n",
        "\n",
        "## RNN-based models {.smaller}\n",
        "\n",
        "<br>\n",
        "\n",
        "We examine five types of RNN-based models:\n",
        "\n",
        "1. Standard RNN\n",
        "2. RNN with LSTM\n",
        "3. RNN with GRU\n",
        "4. TCN\n",
        "5. Dilated RNN\n",
        "\n",
        "<br>\n",
        "\n",
        "This class of model is our top performer among deep learning models. \n",
        "\n",
        "\n",
        "## RNN-based models {.smaller}\n",
        "\n",
        "<br>\n",
        "\n",
        "LSTM and GRU leverage gating mechanisms, distinguishing them from standard RNNs by addressing the vanishing gradient issue and facilitating long-term dependency capture.\n",
        "\n",
        "<br>\n",
        "\n",
        "Dilated RNN employs time dilation (skipped steps) for efficient long-sequence capture without parameter increase.\n",
        "\n",
        "<br>\n",
        "\n",
        "TCNs, on the other hand, utilize dilated causal convolutions, offering efficient long-term dependency management and enhanced parallelism.\n",
        "\n",
        "## \n"
      ],
      "id": "34f250b2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-rnn\n",
        "#| fig-cap: RNN models\n",
        "\n",
        "\n",
        "# Load your data\n",
        "data = pd.read_csv('../presentation/01_data/tidy_results.csv')\n",
        "\n",
        "# Convert 'ds' to datetime\n",
        "data['ds'] = pd.to_datetime(data['ds'])\n",
        "\n",
        "# Define the models to plot\n",
        "models_to_plot = ['AutoDilatedRNN', 'AutoGRU',\n",
        "                  'AutoRNN', 'AutoTCN', 'AutoLSTM']\n",
        "\n",
        "# Filter data for selected models\n",
        "filtered_data = data[data['Model'].isin(models_to_plot)]\n",
        "\n",
        "# Group by model and select every third row\n",
        "grouped = filtered_data.groupby('Model')\n",
        "\n",
        "selected_data = pd.concat([group.iloc[::3] for _, group in grouped])\n",
        "\n",
        "# Map models to colors\n",
        "model_color_dict = {\n",
        "    'AutoDilatedRNN': 'red',\n",
        "    'AutoGRU': 'green',\n",
        "    'AutoRNN': 'blue',\n",
        "    'AutoTCN': 'cyan',\n",
        "    'AutoLSTM': 'purple'\n",
        "}\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Change this line to set x ticks every year\n",
        "ax.xaxis.set_major_locator(YearLocator())\n",
        "\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "\n",
        "ax.tick_params(which=\"major\", width=1.0)\n",
        "ax.tick_params(which=\"major\", length=10)\n",
        "ax.tick_params(which=\"minor\", width=1.0, labelsize=10)\n",
        "ax.tick_params(which=\"minor\", length=5, labelsize=10, labelcolor=\"0.25\")\n",
        "\n",
        "ax.set_ylabel(\"Estimate\", weight=\"medium\")\n",
        "ax.set_xlabel(\"Date\", weight=\"medium\")\n",
        "\n",
        "for model in selected_data['Model'].unique():\n",
        "    subset = selected_data[selected_data['Model'] == model]\n",
        "    ax.scatter(subset['ds'], subset['Estimate'], s=10, color=model_color_dict[model],\n",
        "               edgecolor=model_color_dict[model], linewidth=1, zorder=-20, alpha=0.3)\n",
        "    ax.plot(subset['ds'], subset['Estimate'],\n",
        "            c=model_color_dict[model], linewidth=1, alpha=0.5, label=model)\n",
        "\n",
        "# Display the legend\n",
        "ax.legend(frameon=False)\n",
        "\n",
        "plt.show()"
      ],
      "id": "fig-rnn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformer models\n",
        "\n",
        "<center>![](02_figures/transformer.svg){height=\"50%\"}</center>\n",
        "\n",
        "## Transformer models {.smaller}\n",
        "\n",
        "<br>\n",
        "\n",
        "We examine three transformer-based models:\n",
        "\n",
        "1. Temporal Fusion Transformer\n",
        "2. Informer\n",
        "3. Autoformer\n",
        "\n",
        "<br>\n",
        "\n",
        "These models, unlike RNNs, utilize self-attention, enhancing dependency management and parallel processing.\n",
        "\n",
        "<br>\n",
        "\n",
        "Despite this, their high computational demands, large dataset requirements, and difficulty with sequential data may limit their time series forecasting efficacy.\n",
        "\n",
        "##\n"
      ],
      "id": "3d46039f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-transformer\n",
        "#| fig-cap: Transformer models\n",
        "\n",
        "\n",
        "# Load your data\n",
        "data = pd.read_csv('../presentation/01_data/tidy_results.csv')\n",
        "\n",
        "# Convert 'ds' to datetime\n",
        "data['ds'] = pd.to_datetime(data['ds'])\n",
        "\n",
        "# Define the models to plot\n",
        "models_to_plot = ['AutoTFT', 'AutoInformer',\n",
        "                  'AutoAutoformer']\n",
        "\n",
        "# Filter data for selected models\n",
        "filtered_data = data[data['Model'].isin(models_to_plot)]\n",
        "\n",
        "# Group by model and select every third row\n",
        "grouped = filtered_data.groupby('Model')\n",
        "\n",
        "selected_data = pd.concat([group.iloc[::3] for _, group in grouped])\n",
        "\n",
        "# Map models to colors\n",
        "model_color_dict = {\n",
        "    'AutoTFT': 'red',\n",
        "    'AutoInformer': 'green',\n",
        "    'AutoAutoformer': 'blue',\n",
        "}\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Change this line to set x ticks every year\n",
        "ax.xaxis.set_major_locator(YearLocator())\n",
        "\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "\n",
        "ax.tick_params(which=\"major\", width=1.0)\n",
        "ax.tick_params(which=\"major\", length=10)\n",
        "ax.tick_params(which=\"minor\", width=1.0, labelsize=10)\n",
        "ax.tick_params(which=\"minor\", length=5, labelsize=10, labelcolor=\"0.25\")\n",
        "\n",
        "ax.set_ylabel(\"Estimate\", weight=\"medium\")\n",
        "ax.set_xlabel(\"Date\", weight=\"medium\")\n",
        "\n",
        "for model in selected_data['Model'].unique():\n",
        "    subset = selected_data[selected_data['Model'] == model]\n",
        "    ax.scatter(subset['ds'], subset['Estimate'], s=10, color=model_color_dict[model],\n",
        "               edgecolor=model_color_dict[model], linewidth=1, zorder=-20, alpha=0.3)\n",
        "    ax.plot(subset['ds'], subset['Estimate'],\n",
        "            c=model_color_dict[model], linewidth=1, alpha=0.5, label=model)\n",
        "\n",
        "# Display the legend\n",
        "ax.legend(frameon=False)\n",
        "\n",
        "plt.show()"
      ],
      "id": "fig-transformer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Results \n",
        "\n",
        "## Model Performance: Average Monthly Nowcast, Sorted by RMSE {style=\"font-size:17px\"}\n",
        "\n",
        "<!-- html table generated in R 4.3.0 by xtable 1.8-4 package -->\n",
        "<!-- Fri Jun 23 20:51:14 2023 -->\n",
        "|     | Bias | RMSE | MAE | MAPE | U2  | Bias Prop. | Var. Prop. | Cov. Prop. |\n",
        "| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
        "| DilatedRNN | 0.43 | 0.86 | 0.61 | 107.59 | 0.54 | 0.25 | 0.16 | 0.59 |\n",
        "| LSTM | 0.40 | 0.89 | 0.59 | 100.75 | 0.52 | 0.20 | 0.24 | 0.55 |\n",
        "| RNN | 0.41 | 0.93 | 0.61 | 101.24 | 0.55 | 0.20 | 0.24 | 0.56 |\n",
        "| GRU | 0.46 | 1.02 | 0.67 | 114.39 | 0.61 | 0.21 | 0.35 | 0.44 |\n",
        "| DFM Global | -0.14 | 1.02 | 0.71 | 115.06 | 0.72 | 0.02 | 0.02 | 0.96 |\n",
        "| SVR Linear | -0.10 | 1.15 | 0.74 | 121.20 | 0.70 | 0.01 | 0.27 | 0.72 |\n",
        "| SVR Sigmoid | -0.08 | 1.25 | 0.81 | 108.77 | 0.68 | 0.00 | 0.47 | 0.53 |\n",
        "| DFM Blocked | -0.15 | 1.34 | 0.79 | 117.14 | 0.58 | 0.01 | 0.02 | 0.97 |\n",
        "| SVR RBF | 0.15 | 1.35 | 0.92 | 127.70 | 0.76 | 0.01 | 0.71 | 0.28 |\n",
        "| DFM\\_lasso\\_global | -0.16 | 1.44 | 0.80 | 92.40 | 0.51 | 0.01 | 0.31 | 0.68 |\n",
        "| DFM\\_lasso\\_blocked | -0.18 | 1.61 | 0.81 | 109.57 | 0.55 | 0.01 | 0.08 | 0.91 |\n",
        "| DFM\\_lm\\_global | -0.45 | 1.66 | 1.11 | 132.48 | 0.77 | 0.07 | 0.08 | 0.85 |\n",
        "| DFM\\_lm\\_blocked  | -0.31 | 1.68 | 0.87 | 98.37 | 0.63 | 0.03 | 0.02 | 0.95 |\n",
        "| TCN | 0.52 | 1.99 | 0.99 | 113.40 | 0.78 | 0.07 | 0.75 | 0.18 |\n",
        "| Informer | 0.15 | 2.99 | 1.43 | 112.38 | 0.97 | 0.00 | 0.68 | 0.32 |\n",
        "| TFT | 0.07 | 3.03 | 1.41 | 96.10 | 0.87 | 0.00 | 0.54 | 0.46 |\n",
        "| NBEATS | 0.12 | 3.05 | 1.69 | 158.31 | 0.93 | 0.00 | 0.29 | 0.71 |\n",
        "| NHITS | 0.14 | 3.33 | 1.86 | 154.64 | 0.97 | 0.00 | 0.04 | 0.96 |\n",
        "| NBEATSx | -0.03 | 3.85 | 2.19 | 172.27 | 1.04 | 0.00 | 0.01 | 0.99 |\n",
        "| MLP | 0.08 | 4.09 | 2.03 | 139.90 | 0.98 | 0.00 | 0.01 | 0.99 |\n",
        "| Naive | 0.00 | 4.62 | 2.24 | 152.15 | 1.00 | 0.00 | 0.00 | 1.00 |\n",
        "| Autoformer | -2.59 | 10.49 | 4.27 | 501.43 | 7.52 | 0.06 | 0.42 | 0.52 |\n",
        "\n",
        "## Model Performance: Final Month of Quarter, Sorted by RMSE {style=\"font-size:17px\"}\n",
        "\n",
        "<!-- html table generated in R 4.3.0 by xtable 1.8-4 package -->\n",
        "<!-- Fri Jun 23 21:02:50 2023 -->\n",
        "|     | Bias | RMSE | MAE | MAPE | U2  | Bias Prop. | Var. Prop. | Cov. Prop. |\n",
        "| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
        "| DFM Global | 0.12 | 0.74 | 0.54 | 106.35 | 0.59 | 0.03 | 0.09 | 0.88 |\n",
        "| DFM\\_lm\\_global | 0.21 | 0.79 | 0.54 | 90.45 | 0.48 | 0.07 | 0.04 | 0.89 |\n",
        "| DFM Blocked | 0.15 | 0.84 | 0.59 | 115.70 | 0.54 | 0.03 | 0.07 | 0.90 |\n",
        "| RNN | 0.24 | 0.86 | 0.56 | 83.97 | 0.41 | 0.08 | 0.58 | 0.34 |\n",
        "| DilatedRNN | 0.34 | 0.88 | 0.64 | 84.65 | 0.41 | 0.15 | 0.31 | 0.54 |\n",
        "| SVR Sigmoid | 0.04 | 0.88 | 0.62 | 80.97 | 0.60 | 0.00 | 0.43 | 0.57 |\n",
        "| SVR Linear | 0.01 | 0.93 | 0.60 | 108.01 | 0.64 | 0.00 | 0.19 | 0.81 |\n",
        "| GRU | 0.25 | 1.06 | 0.74 | 97.64 | 0.50 | 0.06 | 0.67 | 0.28 |\n",
        "| TCN | 0.36 | 1.07 | 0.65 | 95.19 | 0.51 | 0.11 | 0.43 | 0.46 |\n",
        "| LSTM | 0.29 | 1.13 | 0.64 | 74.43 | 0.45 | 0.06 | 0.67 | 0.26 |\n",
        "| SVR RBF | 0.30 | 1.46 | 0.88 | 119.53 | 0.75 | 0.04 | 0.76 | 0.20 |\n",
        "| DFM\\_lm\\_blocked | -0.09 | 1.51 | 0.90 | 78.04 | 0.65 | 0.00 | 0.41 | 0.58 |\n",
        "| DFM\\_lasso\\_blocked | 0.32 | 1.94 | 1.02 | 101.59 | 0.69 | 0.03 | 0.80 | 0.17 |\n",
        "| DFM\\_lasso\\_global | 0.05 | 2.02 | 1.06 | 98.01 | 0.61 | 0.00 | 0.67 | 0.33 |\n",
        "| Informer | 0.20 | 3.01 | 1.37 | 95.55 | 1.00 | 0.00 | 0.65 | 0.35 |\n",
        "| TFT | 0.00 | 3.13 | 1.47 | 105.24 | 0.86 | 0.00 | 0.44 | 0.56 |\n",
        "| NBEATS | 0.21 | 3.21 | 1.89 | 184.95 | 2.31 | 0.00 | 0.17 | 0.83 |\n",
        "| NHITS | 0.37 | 3.36 | 1.90 | 160.45 | 0.92 | 0.01 | 0.02 | 0.97 |\n",
        "| MLP | 0.13 | 3.67 | 1.98 | 143.29 | 1.00 | 0.00 | 0.05 | 0.95 |\n",
        "| NBEATSx | -0.05 | 3.91 | 2.20 | 178.69 | 1.04 | 0.00 | 0.00 | 1.00 |\n",
        "| Naive | 0.00 | 4.62 | 2.24 | 152.15 | 1.00 | 0.00 | 0.00 | 1.00 |\n",
        "| Autoformer | -7.59 | 29.83 | 9.35 | 1202.56 | 22.44 | 0.06 | 0.75 | 0.19 |\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "## Conclusion {.smaller}\n",
        "\n",
        "<br>\n",
        "\n",
        "DFMs, RNN-based models and SVRs do the best in terms of nowcasting performance. \n",
        "\n",
        "<br>\n",
        "\n",
        "Newer MLP and transformer based models do markedly worse. \n",
        "\n",
        "<br>\n",
        "\n",
        "The latter class of models take the longest to tune. Computational cost is high. \n",
        "\n",
        "<br>\n",
        "\n",
        "Our prediction is that these models will become valuable in future, but greater investment required in trying to understand how they apply to time series forecasting. \n",
        "\n",
        "\n",
        "# References"
      ],
      "id": "95b12a7f"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}